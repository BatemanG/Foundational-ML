## Research Notes: Module 6 - Unsupervised Learning

**Objective:** To understand methods for learning from unlabeled data $D = \{x_i\}_{i=1}^N$. The goal is to discover hidden structure, such as clusters or the underlying data density.

**References:**
* (8) Slides - Module 6 - Unsupervised Learning.pdf

---

### 1. The Unsupervised Learning Problem

* **Input:** An unlabeled dataset $D = \{x_1, \dots, x_N\}$, where $x_i \in \mathbb{R}^d$.
* **Goal:** Discover interesting, latent structure in the data, without an explicit target $y$.
* **Tasks:**
    * **Clustering:** Grouping similar data points together.
    * **Density Estimation:** Finding the underlying probability distribution $p(x)$ from which the data was sampled.
    * **Dimensionality Reduction:** Projecting the data onto a lower-dimensional subspace while preserving its essential structure (e.g., PCA).

---

### 2. Clustering: K-Means

**Goal:** Partition $N$ data points into $K$ disjoint clusters, $C = \{C_1, \dots, C_K\}$.

**Formal Objective:**
K-Means is an optimization algorithm that aims to minimize the **within-cluster sum of squares (WCSS)**, also known as *inertia*. We want to find the cluster assignments $r_{nk}$ (a binary indicator, 1 if $x_n \in C_k$, 0 otherwise) and cluster centroids $\mu_k$ that minimize:
$$
J(r, \mu) = \sum_{n=1}^N \sum_{k=1}^K r_{nk} ||x_n - \mu_k||^2
$$
This objective is NP-hard to solve globally. K-Means uses an iterative refinement approach (a form of **Expectation-Maximization**) that guarantees convergence to a local minimum.

**Algorithm (Iterative Optimization):**
1.  **Initialize:** Randomly select $K$ data points from $D$ to be the initial centroids $\{\mu_1, \dots, \mu_K\}$.
2.  **Repeat until convergence:**
    * **Assignment Step (E-Step):** Assign each data point $x_n$ to the cluster of its *nearest* centroid (a "hard" assignment). This minimizes $J$ with respect to $r_{nk}$, holding $\mu_k$ fixed.
        $$
        r_{nk} = \begin{cases} 1 & \text{if } k = \arg\min_j ||x_n - \mu_j||^2 \\ 0 & \text{otherwise} \end{cases}
        $$
    * **Update Step (M-Step):** Recompute each centroid $\mu_k$ as the *mean* of all data points assigned to it. This minimizes $J$ with respect to $\mu_k$, holding $r_{nk}$ fixed.
        $$
        \mu_k \leftarrow \frac{\sum_{n=1}^N r_{nk} x_n}{\sum_{n=1}^N r_{nk}}
        $$
3.  **Stop:** When cluster assignments $r_{nk}$ (or centroids $\mu_k$) no longer change.

**Limitations:**
* Requires $K$ to be specified in advance.
* Is sensitive to the initial choice of centroids. (Standard practice: run multiple times with random initializations and pick the one with the lowest final $J$).
* Assumes clusters are convex and isotropic (spherical).

---

### 3. Density Estimation: Parametric vs. Non-Parametric

**Goal:** Given data $D = \{x_i\}_{i=1}^N$, estimate the underlying probability density function $p(x)$.

#### 3.1. Parametric Approach: Gaussian Mixture Models (GMM)

We *assume* the data is drawn from a specific model form, in this case a mixture of $K$ Gaussians.

**Formal Model:**
The density $p(x)$ is a weighted sum of $K$ Gaussian components:
$$
p(x \mid \theta) = \sum_{k=1}^K \phi_k \mathcal{N}(x \mid \mu_k, \Sigma_k)
$$
* **Parameters ($\theta$):** $\theta = \{\phi_k, \mu_k, \Sigma_k\}_{k=1}^K$.
* **Mixing Coefficients ($\phi_k$):** The prior probability $p(k)$ of a point belonging to cluster $k$. Satisfies $\phi_k \ge 0$ and $\sum_{k=1}^K \phi_k = 1$.
* **Components ($\mathcal{N}(\cdot)$):** Each component $k$ is a Gaussian with its own mean $\mu_k$ and covariance matrix $\Sigma_k$.

**Objective Function (Log-Likelihood):**
We cannot "see" which component $k$ generated a point $x_i$ (this is a **latent variable** problem). We find parameters $\theta^*$ that maximize the **log-likelihood** of the observed data:
$$
\mathcal{L}(\theta) = \log p(X \mid \theta) = \log \prod_{i=1}^N p(x_i \mid \theta) = \sum_{i=1}^N \log \left( \sum_{k=1}^K \phi_k \mathcal{N}(x_i \mid \mu_k, \Sigma_k) \right)
$$
Direct maximization of this objective is analytically intractable.

**Expectation-Maximization (EM) for GMMs:**
The EM algorithm is a general technique for finding maximum likelihood estimates in latent variable models. It iterates two steps:

1.  **Initialize:** Guess initial parameters $\theta^{old} = \{\phi_k, \mu_k, \Sigma_k\}$.
2.  **Repeat until convergence:**
    * **E-Step (Expectation):** Compute the "responsibilities" $\gamma(z_{ik})$. This is the posterior probability $p(k \mid x_i, \theta^{old})$ that data point $x_i$ was generated by component $k$ (a "soft" assignment).
        $$
        r_{ik} := \gamma(z_{ik}) = \frac{\phi_k \mathcal{N}(x_i \mid \mu_k, \Sigma_k)}{\sum_{j=1}^K \phi_j \mathcal{N}(x_i \mid \mu_j, \Sigma_j)}
        $$
    * **M-Step (Maximization):** Update the parameters $\theta$ to get $\theta^{new}$, using the responsibilities $\gamma(z_{ik})$ as soft weights for each data point.
        
        Let $N_k = \sum_{i=1}^N \gamma(z_{ik})$ be the "effective number" of points in cluster $k$.
        
        $$
        \phi_k^{new} \leftarrow \frac{N_k}{N}
        $$
        $$
        \mu_k^{new} \leftarrow \frac{1}{N_k} \sum_{i=1}^N \gamma(z_{ik}) x_i
        $$
        $$
        \Sigma_k^{new} \leftarrow \frac{1}{N_k} \sum_{i=1}^N \gamma(z_{ik}) (x_i - \mu_k^{new})(x_i - \mu_k^{new})^T
        $$
3.  **Stop:** When the log-likelihood or parameters stop changing significantly.

**GMM vs. K-Means:**
K-Means is a special, "hard" version of GMM-EM where:
1.  **Assignment:** Responsibilities $\gamma(z_{ik})$ are "hard" (either 0 or 1).
2.  **Covariance:** Clusters are assumed to be spherical and uniform ($\Sigma_k = \sigma^2 I$).
GMM is more flexible, allowing for non-spherical (ellipsoidal) clusters of different sizes and orientations.

#### 3.2. Non-Parametric Approach: Kernel Density Estimation (KDE)

KDE makes no assumption about the *form* of $p(x)$ (e.g., that it's Gaussian). It builds the estimate directly from the data.

**Problem:** Estimate the density $p(x)$ in a small region $R$ around $x$.
* Let $R$ be a region (e.g., a small sphere) with volume $V$.
* The probability of a sample falling in $R$ is $P = \int_R p(x') dx'$.
* If $R$ is small, $p(x)$ is roughly constant inside, so $P \approx p(x) V$.
* From $N$ data points, let $k$ be the number that fall inside $R$.
* From observation, $P \approx \frac{k}{N}$.
* Combining these: $p(x) V \approx \frac{k}{N} \implies p(x) \approx \frac{k}{NV}$.

**Kernel Density Estimation (KDE):**
This is a generalization of the histogram. We define the region $R$ using a **kernel function** $K(u)$, which is a smooth, non-negative function that integrates to 1 (e.g., a Gaussian kernel).

Let the kernel function $K(\cdot)$ be centered at each data point $x_i$. The general form is:
$$
p(x) = \frac{1}{N} \sum_{i=1}^N K_h(x - x_i) = \frac{1}{Nh} \sum_{i=1}^N K\left(\frac{x-x_i}{h}\right)
$$
* **Kernel ($K$):** A function defining the "shape" of influence for each data point (e.g., a standard Gaussian $\mathcal{N}(0, 1)$).
* **Bandwidth ($h$):** A hyperparameter that controls the "width" of the kernel. This is the most critical parameter to choose.
    * **Small $h$:** Leads to a spiky, high-variance estimate (overfitting).
    * **Large $h$:** Leads to an over-smoothed, high-bias estimate (underfitting).

**Gaussian KDE:**
If we choose the kernel $K$ to be a Gaussian, the density estimate $p(x)$ becomes a *mixture of $N$ Gaussians*, one centered on each data point $x_i$, with a shared covariance $\Sigma = h^2 I$:
$$
p(x) = \frac{1}{N} \sum_{i=1}^N \mathcal{N}(x \mid x_i, h^2 I)
$$
This is a GMM where $\phi_k = 1/N$, $\mu_k = x_i$, and $\Sigma_k = h^2 I$. It is a non-parametric method because the number of parameters ($N$ means and $N$ mixing weights) grows with the dataset size $N$.



### 2. Hyperparameter Tuning for GMMs The primary hyperparameter for a GMM is also the number of components, $K$.

#### 2.1. Bayesian Information Criterion (BIC)

Because GMMs are a probabilistic model, we can use information criteria to select $K$. The BIC is a common choice that penalizes model complexity **Goal:** Minimize the BIC score.* **BIC Equation:** $$ BIC_K = \mathcal{L}(K) + \frac{P_K}{2} \log(N) $$ * $\mathcal{L}(K) = - \sum_{i=1}^N \log p(x_i \mid \theta_K)$ is the **Negative Log-Likelihood** of the data given the *best* GMM model with $K$ components.

This term *encourages fitting the data* (it gets smaller as the fit improves).
$P_K$ is the number of independent parameters in the model with $K$ components. 
$N$ is the number of data points.  $\frac{P_K}{2} \log(N)$ is the **complexity penalty**.

